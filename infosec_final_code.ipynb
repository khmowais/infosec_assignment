{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Implementation of maxout like ConvNet + FGSM adversarial training for MNIST\n",
        "#Requirements: torch, torchvision, numpy, matplotlib, pandas\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "#Basic config\n",
        "\n",
        "seed = 1234\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "#Hyperparameters\n",
        "batch_size = 128\n",
        "test_batch_size = 1000\n",
        "epochs = 18\n",
        "lr = 0.01\n",
        "momentum = 0.9\n",
        "weight_decay = 1e-4\n",
        "\n",
        "#FGSM settings\n",
        "epsilon = 0.25            #paper commonly used 0.25 for MNIST in [0,1] scale\n",
        "alpha_mix = 0.5           #mix clean and adv loss like paper\n",
        "\n",
        "output_dir = \"outputs_mnist_maxout\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "#pin_memory for GPU\n",
        "pin_mem = True if torch.cuda.is_available() else False\n",
        "num_workers = 2 if torch.cuda.is_available() else 0\n",
        "\n",
        "\n",
        "#Data\n",
        "transform = transforms.Compose([transforms.ToTensor()])  #values in [0,1]\n",
        "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_mem)\n",
        "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_mem)\n",
        "\n",
        "\n",
        "#Maxout helpers\n",
        "class MaxoutConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, pieces=2):\n",
        "        super().__init__()\n",
        "        self.pieces = pieces\n",
        "        #produce out_channels * pieces channels then do max over pieces\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels * pieces, kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)  #shape (B, out*pieces, H, W)\n",
        "        B, C, H, W = out.shape\n",
        "        #reshape to (B, out, pieces, H, W) then max over pieces\n",
        "        out = out.view(B, C // self.pieces, self.pieces, H, W)\n",
        "        out, _ = out.max(dim=2)\n",
        "        return out\n",
        "\n",
        "class MaxoutLinear(nn.Module):\n",
        "    \"\"\"\n",
        "    Linear that does maxout in the same way: produce out*pieces dims then max over pieces.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, pieces=2):\n",
        "        super().__init__()\n",
        "        self.pieces = pieces\n",
        "        self.lin = nn.Linear(in_features, out_features * pieces)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.lin(x)  #shape (B, out*pieces)\n",
        "        B, C = out.shape\n",
        "        out = out.view(B, C // self.pieces, self.pieces)\n",
        "        out, _ = out.max(dim=2)\n",
        "        return out\n",
        "\n",
        "\n",
        "#Model: small Maxout-like conv net (student style)\n",
        "class MaxoutConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #conv layers with maxout pieces=2\n",
        "        self.conv1 = MaxoutConv2d(1, 64, kernel_size=5, padding=2, pieces=2)   #-> 64 channels\n",
        "        self.pool1 = nn.MaxPool2d(2,2)  #28->14\n",
        "        self.conv2 = MaxoutConv2d(64, 128, kernel_size=5, padding=2, pieces=2) #-> 128 channels\n",
        "        self.pool2 = nn.MaxPool2d(2,2)  #14->7\n",
        "        #flatten -> fc maxout\n",
        "        self.fc1 = MaxoutLinear(128 * 7 * 7, 1024, pieces=2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(1024, 10)  #final linear softmax\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)    #paper uses maxout nonlinearity then often uses dropout; keep ReLU after maxout for stability\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "#FGSM attack (keeps gradients only where needed)\n",
        "def fgsm_attack(model, loss_fn, images, labels, eps):\n",
        "    \"\"\"\n",
        "    Generate FGSM adversarial examples for a batch.\n",
        "    Caller must ensure gradients are enabled (no torch.no_grad active).\n",
        "    \"\"\"\n",
        "    images_adv = images.clone().detach().to(device)\n",
        "    images_adv.requires_grad = True\n",
        "    outputs = model(images_adv)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "    data_grad = images_adv.grad.data\n",
        "    perturbed = images_adv + eps * data_grad.sign()\n",
        "    perturbed = torch.clamp(perturbed, 0.0, 1.0)\n",
        "    return perturbed.detach()\n",
        "\n",
        "#Training + evaluation\n",
        "def train_epoch(model, loader, optimizer, loss_fn, adv_training=False, eps=0.25, alpha=0.5):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device); yb = yb.to(device)\n",
        "        if not adv_training:\n",
        "            optimizer.zero_grad()\n",
        "            out = model(xb)\n",
        "            loss = loss_fn(out, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * xb.size(0)\n",
        "        else:\n",
        "            #create adversarial examples on the fly (must enable grad while generating)\n",
        "            #generate FGSM using current model params\n",
        "            #ensure gradients are computed just for adv creation\n",
        "            with torch.enable_grad():\n",
        "                xb_adv = fgsm_attack(model, loss_fn, xb, yb, eps)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            out_clean = model(xb)\n",
        "            out_adv = model(xb_adv)\n",
        "            loss = alpha * loss_fn(out_clean, yb) + (1.0 - alpha) * loss_fn(out_adv, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * xb.size(0)\n",
        "\n",
        "    return running_loss / len(loader.dataset)\n",
        "\n",
        "def test_model(model, loader, loss_fn, eps=None):\n",
        "    \"\"\"\n",
        "    Evaluate model on clean data (eps=None) or FGSM adversarial data (eps provided).\n",
        "    FGSM generation is done with torch.enable_grad() (not inside torch.no_grad()).\n",
        "    Forward passes for final metric use torch.no_grad() to save memory.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    total_loss = 0.0\n",
        "    confs = []\n",
        "\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device); yb = yb.to(device)\n",
        "        if eps is not None:\n",
        "            #generate adversarial batch with enabled grad\n",
        "            with torch.enable_grad():\n",
        "                xb_eval = fgsm_attack(model, loss_fn, xb, yb, eps)\n",
        "        else:\n",
        "            xb_eval = xb\n",
        "\n",
        "        #now evaluate without grad\n",
        "        with torch.no_grad():\n",
        "            outputs = model(xb_eval)\n",
        "            loss = loss_fn(outputs, yb)\n",
        "            total_loss += loss.item() * xb_eval.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total += xb_eval.size(0)\n",
        "\n",
        "            #confidences on mistaken examples\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            mistaken = (preds != yb).nonzero(as_tuple=False)\n",
        "            if mistaken.nelement() != 0:\n",
        "                #handle both single and multiple indices\n",
        "                mi = mistaken.view(-1).tolist()\n",
        "                for i in mi:\n",
        "                    confs.append(float(probs[i, preds[i]].cpu().item()))\n",
        "\n",
        "    acc = 100.0 * correct / total\n",
        "    avg_loss = total_loss / total\n",
        "    avg_conf_mistake = np.mean(confs) if len(confs) > 0 else None\n",
        "    return avg_loss, acc, avg_conf_mistake\n",
        "\n",
        "\n",
        "#Experiment runner\n",
        "def run_experiment(adv_train=False, epochs=10, prefix=\"baseline\"):\n",
        "    model = MaxoutConvNet().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    history = {\"train_loss\": [], \"test_acc_clean\": [], \"test_acc_adv\": [], \"adv_conf\": []}\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        t0 = datetime.now()\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, loss_fn, adv_training=adv_train, eps=epsilon, alpha=alpha_mix)\n",
        "        loss_clean, acc_clean, _ = test_model(model, test_loader, loss_fn, eps=None)\n",
        "        loss_adv, acc_adv, adv_conf = test_model(model, test_loader, loss_fn, eps=epsilon)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"test_acc_clean\"].append(acc_clean)\n",
        "        history[\"test_acc_adv\"].append(acc_adv)\n",
        "        history[\"adv_conf\"].append(adv_conf)\n",
        "\n",
        "        t1 = datetime.now()\n",
        "        print(f\"[{prefix}] Epoch {ep}/{epochs} | train_loss={train_loss:.4f} | clean_acc={acc_clean:.2f}% | adv_acc={acc_adv:.2f}% | time={(t1-t0).seconds}s\")\n",
        "\n",
        "    #save model & history\n",
        "    torch.save(model.state_dict(), os.path.join(output_dir, f\"{prefix}_model.pth\"))\n",
        "    pd.DataFrame(history).to_csv(os.path.join(output_dir, f\"{prefix}_history.csv\"), index=False)\n",
        "\n",
        "    #save accuracy plot 300 DPI\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(range(1, epochs+1), history[\"test_acc_clean\"], label=\"Clean acc\")\n",
        "    plt.plot(range(1, epochs+1), history[\"test_acc_adv\"], label=f\"FGSM eps={epsilon} acc\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy (%)\"); plt.title(f\"{prefix} Accuracies\")\n",
        "    plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, f\"{prefix}_acc.png\"), dpi=300); plt.close()\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "#Main\n",
        "def main():\n",
        "    print(\"Starting baseline training (no adversarial training)...\")\n",
        "    baseline_model, baseline_hist = run_experiment(adv_train=False, epochs=epochs, prefix=\"baseline\")\n",
        "\n",
        "    print(\"\\nStarting adversarial training (mixing clean + FGSM)...\")\n",
        "    adv_model, adv_hist = run_experiment(adv_train=True, epochs=epochs, prefix=\"advtrain\")\n",
        "\n",
        "    #final eval summary\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    bl_clean = test_model(baseline_model, test_loader, loss_fn, eps=None)\n",
        "    bl_adv = test_model(baseline_model, test_loader, loss_fn, eps=epsilon)\n",
        "    at_clean = test_model(adv_model, test_loader, loss_fn, eps=None)\n",
        "    at_adv = test_model(adv_model, test_loader, loss_fn, eps=epsilon)\n",
        "\n",
        "    summary = {\n",
        "        \"Model\": [\"Baseline\", \"Baseline\", \"AdvTrain\", \"AdvTrain\"],\n",
        "        \"Eval\": [\"Clean\", f\"FGSM eps={epsilon}\", \"Clean\", f\"FGSM eps={epsilon}\"],\n",
        "        \"Loss\": [bl_clean[0], bl_adv[0], at_clean[0], at_adv[0]],\n",
        "        \"Accuracy\": [bl_clean[1], bl_adv[1], at_clean[1], at_adv[1]],\n",
        "        \"Avg_conf_on_mistakes\": [bl_clean[2], bl_adv[2], at_clean[2], at_adv[2]]\n",
        "    }\n",
        "    df = pd.DataFrame(summary)\n",
        "    df.to_csv(os.path.join(output_dir, \"final_summary.csv\"), index=False)\n",
        "    print(\"\\nFinal summary:\")\n",
        "    print(df)\n",
        "\n",
        "    #save some sample images (clean vs FGSM from baseline model)\n",
        "    sample_x, sample_y = next(iter(test_loader))\n",
        "    sample_x = sample_x[:8].to(device); sample_y = sample_y[:8].to(device)\n",
        "    with torch.no_grad():\n",
        "        clean_out = baseline_model(sample_x)\n",
        "        clean_preds = clean_out.argmax(dim=1)\n",
        "\n",
        "    with torch.enable_grad():\n",
        "        adv_sample = fgsm_attack(baseline_model, nn.CrossEntropyLoss(), sample_x, sample_y, epsilon)\n",
        "    with torch.no_grad():\n",
        "        adv_out = baseline_model(adv_sample); adv_preds = adv_out.argmax(dim=1)\n",
        "\n",
        "    fig, axes = plt.subplots(2,8, figsize=(12,3))\n",
        "    for i in range(8):\n",
        "        axes[0,i].imshow(sample_x[i].cpu().squeeze(), cmap=\"gray\"); axes[0,i].axis(\"off\")\n",
        "        axes[0,i].set_title(str(int(clean_preds[i].cpu().numpy())))\n",
        "        axes[1,i].imshow(adv_sample[i].cpu().squeeze(), cmap=\"gray\"); axes[1,i].axis(\"off\")\n",
        "        axes[1,i].set_title(str(int(adv_preds[i].cpu().numpy())))\n",
        "    plt.suptitle(\"Top: clean preds | Bottom: FGSM preds\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, \"sample_clean_vs_adv.png\"), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Saved outputs to\", output_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgewOX1EY2YB",
        "outputId": "671b8603-bad1-4c4b-8490-db6b91b3540f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.4MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 505kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.63MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.95MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting baseline training (no adversarial training)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[baseline] Epoch 1/18 | train_loss=0.3308 | clean_acc=97.64% | adv_acc=7.35% | time=17s\n",
            "[baseline] Epoch 2/18 | train_loss=0.0754 | clean_acc=98.55% | adv_acc=15.79% | time=16s\n",
            "[baseline] Epoch 3/18 | train_loss=0.0541 | clean_acc=98.88% | adv_acc=14.84% | time=16s\n",
            "[baseline] Epoch 4/18 | train_loss=0.0429 | clean_acc=98.97% | adv_acc=15.83% | time=16s\n",
            "[baseline] Epoch 5/18 | train_loss=0.0358 | clean_acc=98.96% | adv_acc=14.51% | time=17s\n",
            "[baseline] Epoch 6/18 | train_loss=0.0306 | clean_acc=99.12% | adv_acc=16.89% | time=17s\n",
            "[baseline] Epoch 7/18 | train_loss=0.0271 | clean_acc=99.30% | adv_acc=16.41% | time=16s\n",
            "[baseline] Epoch 8/18 | train_loss=0.0242 | clean_acc=99.18% | adv_acc=22.66% | time=16s\n",
            "[baseline] Epoch 9/18 | train_loss=0.0215 | clean_acc=99.32% | adv_acc=13.78% | time=17s\n",
            "[baseline] Epoch 10/18 | train_loss=0.0199 | clean_acc=99.20% | adv_acc=15.73% | time=16s\n",
            "[baseline] Epoch 11/18 | train_loss=0.0173 | clean_acc=99.18% | adv_acc=23.40% | time=16s\n",
            "[baseline] Epoch 12/18 | train_loss=0.0158 | clean_acc=99.19% | adv_acc=18.42% | time=17s\n",
            "[baseline] Epoch 13/18 | train_loss=0.0141 | clean_acc=99.27% | adv_acc=21.14% | time=16s\n",
            "[baseline] Epoch 14/18 | train_loss=0.0133 | clean_acc=99.34% | adv_acc=25.29% | time=16s\n",
            "[baseline] Epoch 15/18 | train_loss=0.0115 | clean_acc=99.30% | adv_acc=24.30% | time=16s\n",
            "[baseline] Epoch 16/18 | train_loss=0.0113 | clean_acc=99.29% | adv_acc=23.97% | time=16s\n",
            "[baseline] Epoch 17/18 | train_loss=0.0103 | clean_acc=99.28% | adv_acc=20.86% | time=17s\n",
            "[baseline] Epoch 18/18 | train_loss=0.0095 | clean_acc=99.35% | adv_acc=25.74% | time=16s\n",
            "\n",
            "Starting adversarial training (mixing clean + FGSM)...\n",
            "[advtrain] Epoch 1/18 | train_loss=0.8175 | clean_acc=97.50% | adv_acc=73.70% | time=39s\n",
            "[advtrain] Epoch 2/18 | train_loss=0.3668 | clean_acc=98.22% | adv_acc=77.15% | time=39s\n",
            "[advtrain] Epoch 3/18 | train_loss=0.2869 | clean_acc=98.59% | adv_acc=82.38% | time=40s\n",
            "[advtrain] Epoch 4/18 | train_loss=0.2396 | clean_acc=98.86% | adv_acc=89.53% | time=39s\n",
            "[advtrain] Epoch 5/18 | train_loss=0.2139 | clean_acc=99.03% | adv_acc=85.51% | time=39s\n",
            "[advtrain] Epoch 6/18 | train_loss=0.1940 | clean_acc=99.16% | adv_acc=87.98% | time=39s\n",
            "[advtrain] Epoch 7/18 | train_loss=0.1766 | clean_acc=99.17% | adv_acc=89.16% | time=40s\n",
            "[advtrain] Epoch 8/18 | train_loss=0.1638 | clean_acc=99.19% | adv_acc=90.94% | time=39s\n",
            "[advtrain] Epoch 9/18 | train_loss=0.1532 | clean_acc=99.23% | adv_acc=89.43% | time=39s\n",
            "[advtrain] Epoch 10/18 | train_loss=0.1433 | clean_acc=99.23% | adv_acc=90.13% | time=39s\n",
            "[advtrain] Epoch 11/18 | train_loss=0.1350 | clean_acc=99.26% | adv_acc=91.71% | time=40s\n",
            "[advtrain] Epoch 12/18 | train_loss=0.1257 | clean_acc=99.30% | adv_acc=91.99% | time=39s\n",
            "[advtrain] Epoch 13/18 | train_loss=0.1214 | clean_acc=99.24% | adv_acc=91.93% | time=39s\n",
            "[advtrain] Epoch 14/18 | train_loss=0.1131 | clean_acc=99.31% | adv_acc=91.64% | time=39s\n",
            "[advtrain] Epoch 15/18 | train_loss=0.1069 | clean_acc=99.30% | adv_acc=91.39% | time=40s\n",
            "[advtrain] Epoch 16/18 | train_loss=0.1020 | clean_acc=99.29% | adv_acc=91.51% | time=39s\n",
            "[advtrain] Epoch 17/18 | train_loss=0.0928 | clean_acc=99.25% | adv_acc=93.38% | time=39s\n",
            "[advtrain] Epoch 18/18 | train_loss=0.0839 | clean_acc=99.36% | adv_acc=92.89% | time=39s\n",
            "\n",
            "Final summary:\n",
            "      Model           Eval      Loss  Accuracy  Avg_conf_on_mistakes\n",
            "0  Baseline          Clean  0.019171     99.35              0.783265\n",
            "1  Baseline  FGSM eps=0.25  4.346617     25.74              0.845736\n",
            "2  AdvTrain          Clean  0.017825     99.36              0.746665\n",
            "3  AdvTrain  FGSM eps=0.25  0.206644     92.89              0.732863\n",
            "Saved outputs to outputs_mnist_maxout\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ujbZc4t-ZMew"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}